---
title: Graphs
layout: post
category: CS1501
---
In an **undirected graph**, edges are **bidirectional**. In such case, edges are unordered pairs $(A,B)=(B,A)$. However, this is not true for directed graphs. In **directed graphs** edges are directional, which means they are ordered pairs $(A,B)\neq(B,A)$. Vertices that are connected by an edge is  called **adjacent vertices**, or **neighbors**.

Now, let $v=\|V\|$ and $e=\|E\|$. 

Given $v$, the minimum $e$ is 0 as by our definition, the vertices need not have edges. The maximum $e$, however, depends on if self-edges are allowed, and whether the graph is directed or undirected. (Self-edges are when a vertex connects to itself; used in situations such as transition diagrams). For this class, we will assume that directed graphs have self edges, and undirected graphs don't.

For directed graphs, each vertex can be connected to each other vertex. Since we have $v$ vertices, each with $v$ edges, we have a total of $v^2$ edges.

For undirected graphs, each vertex can be connected to each other vertex, but edge $(i,j)=(j,i)$. That means, the total edges is $\frac{1}{2}$ of the total number of vertex pairs. Thus, assuming $v$ vertices, each can connect to $(v-1)$ other edges. Therefore, we have a total of $\frac{1}{2}{v}{(v-1)}$ edges.

When a graph has the maximum number of edges, it is called **complete**. Also a graph is called **sparse** if $e \leq v\lg(v)$, and **dense** as $e$ approaches $\max(e)$. That is $e = \max(e)-\epsilon$.

## Representing Graphs
Trivially, we can represent graphs as a list of vertices and edges. In this we have two types of representation: **static graphs** (graphs which do not change) and **dynamic graphs** (graphs in which we can add/remove). We will primarily focus on static graphs.

In a static graphs, we want to be able to check if two vertices are neighbors; we also want to find a list of neighbors for a given vertex (in-neighbors and out-neighbors for directed graphs). That means, we mostly only care about the connectivity of the graph.

Also, checking if two vertices are neighbors and finding the list of neighbors can both be performed at $O(e)$. Note that the graph takes up $\theta(v+e)$ space in memory.

Often, different representations in space of the same vertex pairs are considered to be the same graph. On a computer, we can use **adjacency matrixes** to represent arbitrary graphs. An adjacency matrix is a square matrix labeled on rows and columns with vertex ids. Specifically, `M[i][j] = 1` if edge $(i,j)$ exists, and 0 otherwise.

By using an adjacency matrix, checking if two vertices can be done in $\theta(1)$ run-time (since array access is constant time). Finding the list of neighbors of a vertex is done at $O(v)$ run-time. However now, initializing takes an additional $O(v^2)$ time. Space, on the other hand is $O(v^2)$ regardless of $e$.
![](adjMatrixImpl.png)
Another method of representing graphs on the computer is **adjacency list**. An adjacency list is an array of linked lists, where each `list[i]` represents neighbors of vertex $i$.
![](adjListImplofGraph.png)


Now, we are only using $\theta(e +v)$ memory. For sparse graphs, this could be much less than $v^2$. It also only takes $\theta(d)$ to find the list of neighbors of a vertex (where $d$ is the degree of the vertex. I.e., the number of neighbors). Again, for sparse graphs, this could be much less than $v$. However, for dense graphs, the nodes will use much more memory than simple location in adjacency matrix. Also it now takes $O(v)$ in the worst case to find one neighbor (since a linked list only gives sequential access to nodes, and our neighbor may be at the end of the list). 

Overall, adjacency matrix tends to be better for dense graphs; while adjacency list tends to be better for sparse graphs. In general, our naive list approach is ineffective as it is even worse than adjacency list (It also does not store redundant edge information for undirected graphs).

## More definitions
+ **Path**: A sequence of adjacent vertices (`ADC`, `ADCB`)
+ **Simple Path**: A path in which no vertices are repeated (`ABCF`)
+ **Simple Cycle**: A simple path with the same first and last vertex (`ADCBA`)
![](graph2.png)
+ **Connected Graph**: A graph in which a path exists between all vertex pairs (The graph above is connected)
	+ Note that in a complete graph, there is an *edge* between A and B for each pair of vertices, while in a connected graph, there is a *path* between A and B.
	+ **Connected Component**: Connected subgraph of a graph (If we were to remove $(AB),(AD),(CF)$, we would have two connected components: $\{A,E,F\},\ \{ B,C,D\}$)
+ **Acyclic Graph**: A graph with no cycles (The graph above has cycles)
+ **Tree**: A connected, acyclic graph (If we were to remove $(AD), (CF)$, it would become a tree with $v-1=5$ edges)
	+ Has exactly $v-1$ edges 

## How to traverse a graph
Unlike linear data structures, it is not obvious how to systematically visit all vertices in a graph. Two famous, well-known traversal methods are: **Breadth First Search** (BFS) and **Depth First Search** (DFS).

### Depth First Search
Depth First Search dives as deep as possible into the graph first, and branches into other directions only when necessary. We've already seen and used this method. Recall how we built the codebook out of the Huffman Trie during Huffman encoding. DFS can be implemented easily using recursion. That is, for each vertex, visit *first* unseen neighbor. Then, we try the next unseen neighbor, and backtrack at dead-ends (vertices with no unseen neighbors). Note that we are assuming that the order that neighbors of a vertex are stored is not specified.
```PSEUDOCODE
DFS(vertex v){
	seen[v] = true //mark v as seen
	visit v //pre-order DFS
	for each unseen neighbor w{
		parent[w]=v
		DFS(w)
		visit v //in-order DFS
	}
	visit v //post-order DFS
}
```

Note that depending on when we visit a vertex(v), we can either have pre-order, in-order, or post-order DFS.


### Breadth First Search
In Breadth First Search, we search all directions evenly, visiting all vertices distance $i$ from the starting point before visiting any vertices at distance $i+1$. This is especially useful when computing the distance between two vertices.

BFS can be easily implemented using a queue. For each vertex visited, we add all of its neighbors to the queue (if not previously added). Vertices that have been seen (i.e., added to the queue) but not yet visited are said to be the fringe. Then we pop head of the queue to be the next visited vertex.
```PSEUDOCODE
Q = new Queue
BFS(vertex v){  //Start at some vertex, V
	add v to Q
	while(Q is not empty){
		w = remove head of Q
		visited[w] = true //mark w as visited
		for each unseen neighbor x{
	        seen[x] = true //mark x as seen
	        parent[x] = w
			add x to Q
		}
	}
}
```

BFS is especially useful for calculating the shortest path between two vertices.
```PSEUDOCODE
Q = new Queue
BFS(vertex v){ // Start at some vertex, V
	add v to Q
	while(Q is not empty){
		w = remove head of Q
		visited[w] = true //mark w as visited
		for each unseen neighbor x {
			seen[x] = true //mark x as seen
	        parent[x] = w
	        distance[x] = distance[w] + 1
			add x to Q
		}
	}
}
```



## Finding connected components
A connected component is a connected subgraph $G'$ with $(V', E')$. To find all connected components, we need a wrapper function around the BFS which loops and continually calls `BFS()` while there are still unseen vertices. 

If the graph is connected, the wrapper function will call DFS or BFS only once, and a **spanning tree** for the graph can be built. If the graph is not connected, the wrapper function would have to call DFS or VFS multiple times. First call of each will terminate with some vertices still unseen, and loop in "search" will iterate to the next unseen vertex, calling `BFS()` again. Each call will yield a **spanning tree** for a **connected component** of the graph.
```PSEUDOCODE
int components = 0
for each vertex v in V{
	if visited[v] = false{
		components++
		Q = new Queue
		BFS(v)
	}
}
BFS(vertex v){
	add v to Q
	component
	while(Q is not empty){
		w = remove head of Q
		visited[w] = true
		component[w] = components
		for each unseen neighbor x{
	        seen[x] = true 
			add x to Q
		}
	}
}

```

## Run-time of Traversals

### BFS Analysis
In BFS, each vertex is added to the queue exactly once and removed exactly once. Thus we have $v$ add/remove operations $\implies O(v)$ for vertex processing. Edges are checked when adding the list of neighbors to the queue, and each edge is checked at most twice, one per edge endpoint. Thus, we have $O(e)$ for edge processing. Therefore, we have total run-time = vertex processing time + edge processing time = $O(v+e)$.

### DFS Analysis
In DFS, each vertex is seen then visited exactly once. Thus, we have $O(v)$ for vertex processing (except when we are (re)visiting a vertex after each child; In this case, vertex processing occurs inside edge processing). Edges are then checked when finding the list of neighbors, with each edge checked at most twice, one per edge endpoint. Thus, we have $O(e)$ for edge processing. Therefore, we have total run-time = vertex processing time + edge processing time = $O(v+e)$.

Notice that since in both DFS and BFS looks at every edge in the graph twice, DFS and BFS have the same run-time at a high level. In both methods, each vertex must be seen and visited, just in different orders. However, run-times differ depending upon how the graphs are represented(adjacency matrix or an adjacency list).

For instance, in adjacency matrix representations, traversals checks each possible edge. Thus we have $O(v^2)$  time for processing edges, and therefore a total run-time of $O(v^2+v)=O(v^2)$. However, in adjacency list representations, traversals only require $O(v+e)$ run-time as discussed. (Note that for a dense graph $v+e=O(v^2)$).

## Biconnected 
A **biconnected graph** has at least *2 distinct paths* (no common edges or vertices except start and end) between all vertex pairs. That is, if one path is disrupted or broken, there is always an alternative way to get there. Any graph that is not biconnected has one or more **articulation points** (vertices, that, if removed, will separate the graph). Any graph that has no articulation points is biconnected. Thus we can determine that a graph is biconnected if we look for, but do not find any articulation points

## Finding Articulation Points of a Graph
We can find articulation points using a variation of DFS. Consider graph edges during DFS. A **tree edge** is an edge crossed when connecting a new vertex to the spanning tree. A **back edge** connected two vertices that were already connected. Back edges are critical in showing biconnectivity since they represent "alternate" paths between vertices in the graph.![](travgrapj.png)
In this tree, $(4,9), (2,6)$ are back edges. We first number the vertices with their (pre-order) traversal order (in black). Then, for each non-root vertex v, find the lowest numbered vertex reachable (not through v's parent) from v. We want to use 0 or more tree edges then at most one back edge. Then, move down the tree looking for a back edge that goes backwards the furthest.

Now, let's define a function `low(v)` which returns the lowest-numbered vertex reachable from `v` using 0 or more spanning tree edges and then **at most one** back edge. `low(v)` is the minimum of `num(v)` (the vertex is reachable from itself), Lowest `num(w)` of all back edges (v, w), and Lowest `low(w)` of all children of v (the lowest-numbered vertex reachable through a child).

Now when we compare `low(w)` and `num(v)`, if `low(w) >= num(v)`, it means that the child has no other way except through its parent to reach at least one other vertex. For example 7 cannot reach 0,1, and 3 without going through 4. Thus, 4 must be an articulation point. In summary, each non-root vertex `v` that has a child `w` with `low(w) >= num(v)` is an articulation point.

### The Algorithm
First, we build a DFS spanning tree. We create back edges when considering a vertex that has already been visited. We label each vertex `v` with two numbers:
+ `num(v)` = pre-order traversal order
	+ `num(v)` is computed as we move down the tree using pre-order DFS
+ `low(v)` = lowest-numbered vertex reachable from v using 0 or more spanning tree edges and then at most one back edge
	+ `low(v)` is computed as we move up the tree
```PSEUDOCODE
int num = 0
DFS(vertex v) {
	num[v] = num++ 
	low[v] = num[v] //initially
	seen[v] = true //mark v as seen
	for each neighbor w{
		if(w unseen){  
		    parent[w] = v
		    DFS(w) //after the call returns low[w] is computed, why?
	         low[v] = min(low[v], low[w])
		} else { //seen neighbor
		if(w != parent[v]) //and not the parent, so back edge
			low[v] = min(low[v], num[w])
		}	
	}
}
```

But what if the root of the spanning tree is an articulation point? That is, what if we start DFS at an articulation point? Clearly, the child of the root cannot possibly reach a vertex with a smaller `num(v)` that the root, since the root as `num(v)=0`. However, if the root of the spanning tree has more than one child, we can determine the root to be an articulation point. This is because, we backtrack only when necessary (in DFS), meaning having two children in the tree means that we couldn't go reach the second child except by going back through the root (implying that the first child and second child have no way of connecting except through the root).

## Graph Compression
In real life, graphs are huge; they take up hundreds, if not thousands of gigabytes. Take for example, graphs on Facebook, or Google Maps. Let us now consider how we could reduce the size of large graphs:

1. Construct a Compressed Sparse Row (CSR) representation of the graph
	+ A CSR representation is when edges array concatenates sorted neighbor lists of all vertices, and offsets array contains the starting index (in the edges array) for the neighbors of vertex `v`.
	+ Notice that the required space of this representation is now $\theta(v + e)$. Assuming we use 4 bytes per vertex and per edge, the total size would be $4v+8e$ bytes.
	+ Can we compute the degree of a vertex using the offsets array?
Running time?

![](mst.png)
2. Difference coding
	+ For each vertex $v$, with a neighbor list $v_1, v_2, v_3...$, store the differences between each two consecutive numbers $(v_1-v),(v_2-v_1),(v_3-v_2)...$.
	+ Now in the example above, we have an array of {1, 3 ,-1, 2, 2 ,-1, 2 -1, 2, 1 -4, 1, 2, -2}
3. Finally, use gamma code to compress the differences
	+ Gamma code is used to compress data in which small values are much more frequent than large values.
	+ To encode an integer $x$, find $T:=$ the largest power of $2<x$. Encode $T$ as $\log T$ zeros followed by `1`. Then, append the remaining $\log T$ binary digits of $x$.
	+ For example, to encode $17: 10001_2$, we calculate $T=16=2^4$ and get the gamma code: `0000 1 0001`.
	+ This works because $2\times \lfloor(\log x)\rfloor+1$ is much smaller than 32 bits if $x$ is small. 

In fact, the goal of graph compression is to make the differences between vertex labels in each neighbor list small such that their gamma codes are less than 32 bits. For example, in web graphs (where each vertex is a web page, and an edge a hyperlink), we can sort pages based on their URL. This is because the web has locality (in that most of the links on a webpage points to other pages in the same domain); thus neighbors will be close to each other in the sorted list, and their differences will be small. In other types of graphs, we can re-label and transform the graphs such that their differences are small (see [Compact Representations of Separable Graphs](https://www.cs.cmu.edu/~guyb/papers/BBK03.pdf)).

## Neighborhood connectivity Problem
Now, consider this problem: We want to keep a set of neighborhoods connected with the minimum cost possible. Our input is a set of neighborhoods and a file with the following format:
```txt
neighborhood i, neighborhood j, cost of connecting the two neighborhoods
```
Our desired output is a set of neighborhood pairs to be connected and a total cost such that:
1. we can go from any neighborhood to any other (**connected**)
2. the total cost should be minimum (**minimal cost**)

Thinking about what data structure to use, a *graph* would be a good choice to show connectivity; but how can we model the costs?

Up until now, we've considered the spatial layouts of graphs as irrelevant. This is called an **unweighted graph**, where all of the edges are equivalent. Now, we want to be able to reason about bandwidth, distance, capacity, etc. of the real world things our graph represents; that is, we want to store information about our edges. Thus, we must put **weights** on the edges to represent these values.
+ In an adjacency matrix, we can substitute the weight for one (zero still means no edge)
+ In an adjacency list, we can add a field to our linked list nodes for weight of the edge
To implement, we could make our weighted graph a subclass of the Graph discussed previously.

In an unweighted graph, we have seen algorithms to determine the **Spanning Tree** of the graph, and the **Shortest Path**. Both DFS and BFS generated the spanning tree during traversal, assuming the graph is connected; the shortest path can be determined from the spanning tree generated by the BFS. However, in a weighted graph, these ideas can be more complex. 

In a weighted graph, this problem is called finding the **minimum spanning tree** and the **weighted shortest path**. A Minimum Spanning Tree (MST) is the spanning tree of a graph whose edge weights sum to the minimum amount. Clearly, a graph has many spanning trees, not all of which are minimum. A Weighted Shortest Path is the path between two vertices whose edge weights sum to the minimum value. Note that now the fewest edges does not necessarily imply the shortest path.

# Prim's Algorithm
One approach in finding the minimum spanning tree is called Prim's algorithm. We first start by initialize `T` (which will store our tree) to contain the starting vertex. Then, while there are vertices that are not in `T`, we find the minimum edge-weight edge that connects a vertex in `T` to a vertex not yet in `T` and add the edge with its vertex to `T`.

A naïve implementation of Prim's algorithm might look at all possible edges, at each step, that could be added at that step, and choose the smallest. That means, for a complete graph(worst-case):
1. First iteration: $v-1$ possible edges (from start vertex)
2. Next iteration: $2(v-2)$ possible edges (first two vertices each have $(v-1)$ edges, but shared edge between them is not considered)
3. Next iteration: $3(v-3)$ possible edges
4. ...
In total, we have $\sum_{i=1}^{v-1}{i\times (v-i)}=\theta(\text{largest term}\times\text{number of terms})=\theta(\frac{v^2}{4}\times (v-1))=\theta(v^3)$  

## Improving Prim's Algorithm
Notice that we need not consider all the remaining edges for each vertex. In fact, we only need to consider the best edge possible for each vertex. We can do this by updating the best edge of each vertex as we add each vertex to `T`.
+ Add start vertex to `T`
+ Search through the neighbors of the added vertex to adjust the parent and best edge arrays as needed
+ Search through the best edge array to find the next addition to `T`
+ Repeat until all vertices added to `T`

### Run-time of Improved Prim's Algorithm
For every vertex we add to `T`, we need to check all of its neighbors to update their best edges as needed. Assuming we use an adjacency matrix, it takes $\theta(v)$ to check the neighbors of a given vertex, $\theta(1)$ to update the parent/best edge arrays, and $\theta(v)$ to pick the next vertex. So, in total we have a running time of $v\times2\cdot\theta(v)=\theta(v^2)$.

If we are using an adjacency list, it takes $\theta(d)$ to check the neighbors of a given vertex, $\theta(1)$ to update parent/best edge arrays, and $\theta(v)$ to pick the next vertex. So similarly, we have a total running time of $v\times\theta(v+d)=\theta(v^2)$.

### Implementation
```
seen, parent, and BestEdge are arrays of size v
Initialize seen to false, parent to -1, and BestEdge to infinity
BestEdge[start] = 0
for i = 0 to v-1{
	Find a vertex w with seen[w] = false
	and BestEdge[w] is the minimum over all unseen vertices
	seen[w] = 1
	for each neighbor x of w{
		if(BestEdge[x] > edge weight of edge (w, x)
			BestEdge[x] = edge weight of (w, x)
			parent[x] = w
	}
The parent array represents the found MST
}
```

### Using Priority Queues (Lazy Prim MST)
If we use a priority queue, we can pick the best edge even faster. Recall that priority queues can remove the min value stored at $\theta(\lg n)$ time (and also add in $\theta(\lg n)$ time). Now, all we have to do is: visit a vertex, add edges coming out of it to a `PQ`, and while there are unvisited vertices, pop from the `PQ` for the next vertex to visit and repeat.

What is the new run-time? Well, in the worst case each edge in the graph is added to the PQ at some point, and all $e$ edges must be removed (although we could stop earlier if we know the tree has already been completed). So we have, $e \times \theta(\lg e) + e \times \theta(\lg e)=\theta(2 \times e\lg e)=\theta(e \lg e)$. ●This algorithm is known as **lazy Prim’s**.

### Eager Prim MST
However, we can do even better. Notice that we do not need to maintain $e$ items in the PQ. Just like the best edge array implementation, we only need the best edge for each vertex (PQ will need to be indexable to update the best edge).

Now, at each step, we do the following: Look at all of the "best" edges for the vertices in T', Add the overall best edge and vertex to T, and Update the "best" edges for the vertices remaining in T', considering now edges from latest added vertex. The (Vertex, Weight) pairs are now stored in the PQ rather than just edges and weight represents the “best” edge candidate to put that vertex into the tree. At each step the overall best vertex (i.e. the one with the smallest edge) is removed from the PQ, then its adjacency list is traversed, and the remaining vertices in the PQ are updated if necessary. The algorithm continues until the PQ is empty.

Notice, we still consider all edges, but now the PQ entries are vertices rather than edges. Thus the run-time for Eager Prim is $\theta(e \lg(v))$.

### Implementation Comparison

| Implementation | Runtime | Space | Requirements |
|----------------|---------|-------|--------------|
|Parent/Best Edge array Prim's | $\theta(v^2)$ | $\theta(v)$||
|Lazy Prim's | $\theta(e \lg e)$ | $\theta (e)$ | Requires PQ|
| Eager Prim's | $\theta(e \lg v)$ | $\theta(v)$ | Requires indexable PQ|

## Kruskal's MST
Another famous MST algorithm is called the **Kruskal's MST**. The idea is that we insert all edges into a PQ, grab the min edge from the PQ that does not create a cycle in the MST, and remove it from the PQ and add it to the MST. Rather than building MST from a single source (as Prim does), Kruskal adds edges potentially throughout the graph. Eventually they connect into a single MST (assuming the graph is connected).

To detect cycles, we can use BFS/DFS at $v+e$ run-time, or use **Union/Find data structure** at $\log v$ (Sedgewick 1.5). We may have to do $e$ iterations for `removeMin`($\log e$) and Cycle detection. In total, we get a runtime of $\theta(e \log e)$. 

In the case of a connected graph, $v-1 \leq e \leq v^2 \implies \log v \leq \log e  \leq 2\log v \implies \log(e)=\theta(\log v)$. Thus, we get a total run-time of $e \log v$ which is the same as Prim's.